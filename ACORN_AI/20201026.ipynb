{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RNN\n",
    " - 순서(이미지와 같은)와 시간성(시계열분석)을 가진데이터에서 특징을 추출하는양\n",
    " - rnn을 구성하는 cell하나는 ffnn망으로 구성된다\n",
    " - cell과 cell을 연결해서 시간적으로 누적되는 특성을 추출한다\n",
    " - rnn은 다양한 망 구성이 가능하다 (압력과 출력의 구성으로 )\n",
    "      - one to one : 바닐라망( 기본망 )\n",
    "      - one to many : captioning\n",
    "      - many to one : 감정분석\n",
    "      - many to many : 번역망이나 비디오 해석\n",
    "      \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처음데이터 특성 : \n",
      " [[ 0.21256007  0.5256222  -0.3534151  -0.91985595 -0.8965427 ]\n",
      " [ 0.99056077  0.7047399  -0.74324226 -0.999991   -0.9868129 ]\n",
      " [ 0.9999307   0.82402647 -0.91312766 -1.         -0.998386  ]\n",
      " [ 0.9988971  -0.99174494 -0.9281864  -0.9999409   0.9999681 ]] 차수 :  (4, 5)\n",
      "두번째데이터 특성 : \n",
      " [[ 0.99999905 -0.79623216 -0.95116675 -1.         -0.99529076]\n",
      " [-0.06942188 -0.92542714 -0.4515368   0.2824346  -0.84856373]\n",
      " [ 0.99983263 -0.932675   -0.86080503 -0.99999905 -0.96854764]\n",
      " [ 0.9750643   0.27966166 -0.6391409  -0.98708534  0.84477115]] 차수 :  (4, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "n_inputs = 3 # 들어온다 1x3 # 셀당 입력 차수\n",
    "n_neurons = 5 # 나간다 1x5 필요한 가중치 3x5 # 셀당 출력 차수\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs]) # None == 4, 4x3\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "# 셀정의\n",
    "# 셀당 필요한 가중치 3x5\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)#출력차수\n",
    "\n",
    "# rnn망 구성 : static_rnn(메모리를 확보), dynamic_rnn(동적으로 메모리확보)\n",
    "# 입력 2x4x3 # 셀수 4\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1], dtype=tf.float32) # 2 \n",
    "\n",
    "# 출력 output_seqs = 2x4x5 \n",
    "# state = 1x5\n",
    "Y0, Y1 = output_seqs # 각각 4x5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 번역 : 나는 학교에 간다 => encoding\n",
    "# A ascii code : 65\n",
    "X0_batch = np.array([[0,1,2], [3,4,5], [6,7,8], [9,0,1]]) # 입력 4x3 # 아스키\n",
    "X1_batch = np.array([[9,8,7], [0,0,0], [6,5,4], [3,2,1]])\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch}) # 처리결과 나온 시퀀스\n",
    "print(\"처음데이터 특성 : \\n\", Y0_val, \"차수 : \", Y0_val.shape)\n",
    "print(\"두번째데이터 특성 : \\n\", Y1_val, \"차수 : \", Y1_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_val : (4, 2, 5)\n",
      "[[[ 0.16713928 -0.23167202 -0.55705845  0.87856007  0.50833905]\n",
      "  [ 0.93700606 -0.9851856  -0.9999999   1.          0.9999807 ]]\n",
      "\n",
      " [[ 0.74557114 -0.8792165  -0.99913025  0.99997157  0.98876274]\n",
      "  [-0.02421216 -0.0790673  -0.9905376   0.99997735  0.99517924]]\n",
      "\n",
      " [[ 0.9421775  -0.9868425  -0.99999857  1.          0.99980414]\n",
      "  [ 0.35380256 -0.5951678  -0.9997788   0.9999991   0.9992617 ]]\n",
      "\n",
      " [[-0.53928727  0.54787964 -0.9991711   0.9998946   0.95357466]\n",
      "  [ 0.36374104 -0.41695455 -0.9587238   0.8786859   0.91251093]]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "다음 모델에서 차수들을 결정하시오\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "n_steps = 2 # 셀수 \n",
    "n_inputs = 3 # 셀당입력\n",
    "n_neurons = 5 # 셀당출력\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) # None : 4\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)\n",
    "\n",
    "# static : for 문을 사용하지 않고 메모리 공간확보\n",
    "# dynamic : for문을 이용해서 계산\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "# outputs : 4x2x5, states : 1x5\n",
    "X_batch = np.array([ # 4x2x3 # 3:셀당입력 # 2:셀수 # 4:배치사이즈\n",
    "    [[0,1,2], [9,8,7]],\n",
    "    [[3,4,5], [3,4,5]],\n",
    "    [[6,7,8], [6,5,4]],\n",
    "    [[9,0,1], [3,2,1]],\n",
    "])\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    output_val = outputs.eval(feed_dict={X:X_batch})\n",
    "print(\"output_val : {}\\n{}\".format(output_val.shape, output_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tf.reset_default_graph() # 그래프 초기화\n",
    "n_steps = 28 # 셀수\n",
    "n_inputs = 28 # 셀당입력\n",
    "    # 28x28 => 이미지한장\n",
    "n_neurons = 150 # 셀당출력\n",
    "    # 특징확대\n",
    "n_outputs = 10 # \n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) # ?, 28, 28\n",
    "y = tf.placeholder(tf.int32, [None]) # ?\n",
    "\n",
    "#RNN망 구성및 특징 추출\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) # 28 => 150\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32) # ?(밑코드 150), 28, 28\n",
    "# outputs : ?x28x150 , states : ? x 150\n",
    "\n",
    "#FFNN 시작\n",
    "#가중치 다 생성\n",
    "logits = tf.layers.dense(states, n_outputs) # fully-connected : 가중치 생략\n",
    "# 가중치를 계산 : ?(밑코드 150)x150 -> 150x10(학습해서 찾은 가중치) -> 150x10\n",
    "# cost function : cross_entropy\n",
    "# 확률값으로 변환 -> 비용값을 계산\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "\n",
    "loss = tf.reduce_mean(xentropy) # 비용 : 배치사이즈에 대한 평균값으로\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss) # 학습\n",
    "correct = tf.nn.in_top_k(logits, y, 1) # 가장 큰수의 인덱스구하고 y(실제값)과 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # 정확도 # 캐스팅:확률값은 소수점까지 계산\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.93333334 Test_accurracy: 0.9257\n",
      "1 Train accuracy: 0.96 Test_accurracy: 0.95\n",
      "2 Train accuracy: 0.96666664 Test_accurracy: 0.9575\n",
      "3 Train accuracy: 0.97333336 Test_accurracy: 0.9601\n",
      "4 Train accuracy: 0.98 Test_accurracy: 0.9619\n",
      "5 Train accuracy: 0.96 Test_accurracy: 0.9613\n",
      "6 Train accuracy: 0.96 Test_accurracy: 0.9687\n",
      "7 Train accuracy: 0.96 Test_accurracy: 0.9605\n",
      "8 Train accuracy: 0.99333334 Test_accurracy: 0.9664\n",
      "9 Train accuracy: 0.96 Test_accurracy: 0.9662\n",
      "10 Train accuracy: 0.97333336 Test_accurracy: 0.9711\n",
      "11 Train accuracy: 0.98 Test_accurracy: 0.9738\n",
      "12 Train accuracy: 0.98 Test_accurracy: 0.9672\n",
      "13 Train accuracy: 0.96666664 Test_accurracy: 0.9709\n",
      "14 Train accuracy: 0.9866667 Test_accurracy: 0.9749\n",
      "15 Train accuracy: 0.99333334 Test_accurracy: 0.9748\n",
      "16 Train accuracy: 0.98 Test_accurracy: 0.975\n",
      "17 Train accuracy: 0.99333334 Test_accurracy: 0.9753\n",
      "18 Train accuracy: 0.99333334 Test_accurracy: 0.9757\n",
      "19 Train accuracy: 0.99333334 Test_accurracy: 0.9736\n",
      "20 Train accuracy: 1.0 Test_accurracy: 0.9738\n",
      "21 Train accuracy: 0.9866667 Test_accurracy: 0.9768\n",
      "22 Train accuracy: 1.0 Test_accurracy: 0.9762\n",
      "23 Train accuracy: 0.96 Test_accurracy: 0.9677\n",
      "24 Train accuracy: 0.99333334 Test_accurracy: 0.9786\n",
      "25 Train accuracy: 1.0 Test_accurracy: 0.9742\n",
      "26 Train accuracy: 1.0 Test_accurracy: 0.972\n",
      "27 Train accuracy: 0.99333334 Test_accurracy: 0.9729\n",
      "28 Train accuracy: 0.9866667 Test_accurracy: 0.9785\n",
      "29 Train accuracy: 0.9866667 Test_accurracy: 0.975\n",
      "30 Train accuracy: 0.9866667 Test_accurracy: 0.9738\n",
      "31 Train accuracy: 0.99333334 Test_accurracy: 0.9775\n",
      "32 Train accuracy: 0.9866667 Test_accurracy: 0.9756\n",
      "33 Train accuracy: 0.99333334 Test_accurracy: 0.972\n",
      "34 Train accuracy: 0.99333334 Test_accurracy: 0.9767\n",
      "35 Train accuracy: 0.98 Test_accurracy: 0.9759\n",
      "36 Train accuracy: 0.99333334 Test_accurracy: 0.9791\n",
      "37 Train accuracy: 0.99333334 Test_accurracy: 0.9787\n",
      "38 Train accuracy: 0.99333334 Test_accurracy: 0.9771\n",
      "39 Train accuracy: 0.99333334 Test_accurracy: 0.9745\n",
      "40 Train accuracy: 0.99333334 Test_accurracy: 0.9754\n",
      "41 Train accuracy: 0.99333334 Test_accurracy: 0.9778\n",
      "42 Train accuracy: 1.0 Test_accurracy: 0.9765\n",
      "43 Train accuracy: 0.98 Test_accurracy: 0.9753\n",
      "44 Train accuracy: 0.9866667 Test_accurracy: 0.9772\n",
      "45 Train accuracy: 0.99333334 Test_accurracy: 0.9782\n",
      "46 Train accuracy: 0.98 Test_accurracy: 0.978\n",
      "47 Train accuracy: 0.9866667 Test_accurracy: 0.9752\n",
      "48 Train accuracy: 1.0 Test_accurracy: 0.9772\n",
      "49 Train accuracy: 0.99333334 Test_accurracy: 0.9774\n",
      "50 Train accuracy: 0.99333334 Test_accurracy: 0.9748\n",
      "51 Train accuracy: 1.0 Test_accurracy: 0.9757\n",
      "52 Train accuracy: 1.0 Test_accurracy: 0.9764\n",
      "53 Train accuracy: 0.98 Test_accurracy: 0.9756\n",
      "54 Train accuracy: 0.9866667 Test_accurracy: 0.9767\n",
      "55 Train accuracy: 1.0 Test_accurracy: 0.9789\n",
      "56 Train accuracy: 0.99333334 Test_accurracy: 0.9781\n",
      "57 Train accuracy: 0.99333334 Test_accurracy: 0.9794\n",
      "58 Train accuracy: 0.98 Test_accurracy: 0.979\n",
      "59 Train accuracy: 0.99333334 Test_accurracy: 0.9757\n",
      "60 Train accuracy: 0.9866667 Test_accurracy: 0.9785\n",
      "61 Train accuracy: 1.0 Test_accurracy: 0.978\n",
      "62 Train accuracy: 0.99333334 Test_accurracy: 0.9783\n",
      "63 Train accuracy: 0.99333334 Test_accurracy: 0.9774\n",
      "64 Train accuracy: 1.0 Test_accurracy: 0.9781\n",
      "65 Train accuracy: 0.98 Test_accurracy: 0.9736\n",
      "66 Train accuracy: 1.0 Test_accurracy: 0.9763\n",
      "67 Train accuracy: 0.9866667 Test_accurracy: 0.9795\n",
      "68 Train accuracy: 1.0 Test_accurracy: 0.977\n",
      "69 Train accuracy: 0.99333334 Test_accurracy: 0.9758\n",
      "70 Train accuracy: 1.0 Test_accurracy: 0.9733\n",
      "71 Train accuracy: 0.9866667 Test_accurracy: 0.9717\n",
      "72 Train accuracy: 1.0 Test_accurracy: 0.9766\n",
      "73 Train accuracy: 0.99333334 Test_accurracy: 0.982\n",
      "74 Train accuracy: 1.0 Test_accurracy: 0.9775\n",
      "75 Train accuracy: 1.0 Test_accurracy: 0.9789\n",
      "76 Train accuracy: 1.0 Test_accurracy: 0.972\n",
      "77 Train accuracy: 1.0 Test_accurracy: 0.9807\n",
      "78 Train accuracy: 0.99333334 Test_accurracy: 0.9783\n",
      "79 Train accuracy: 0.97333336 Test_accurracy: 0.9767\n",
      "80 Train accuracy: 0.99333334 Test_accurracy: 0.9768\n",
      "81 Train accuracy: 1.0 Test_accurracy: 0.9764\n",
      "82 Train accuracy: 1.0 Test_accurracy: 0.9769\n",
      "83 Train accuracy: 0.98 Test_accurracy: 0.9761\n",
      "84 Train accuracy: 0.99333334 Test_accurracy: 0.9808\n",
      "85 Train accuracy: 0.99333334 Test_accurracy: 0.9733\n",
      "86 Train accuracy: 0.94 Test_accurracy: 0.97\n",
      "87 Train accuracy: 1.0 Test_accurracy: 0.9751\n",
      "88 Train accuracy: 1.0 Test_accurracy: 0.9787\n",
      "89 Train accuracy: 1.0 Test_accurracy: 0.9764\n",
      "90 Train accuracy: 0.99333334 Test_accurracy: 0.9781\n",
      "91 Train accuracy: 0.99333334 Test_accurracy: 0.9757\n",
      "92 Train accuracy: 0.99333334 Test_accurracy: 0.9768\n",
      "93 Train accuracy: 0.9866667 Test_accurracy: 0.9792\n",
      "94 Train accuracy: 0.99333334 Test_accurracy: 0.9792\n",
      "95 Train accuracy: 1.0 Test_accurracy: 0.978\n",
      "96 Train accuracy: 1.0 Test_accurracy: 0.9744\n",
      "97 Train accuracy: 0.97333336 Test_accurracy: 0.9701\n",
      "98 Train accuracy: 0.9866667 Test_accurracy: 0.979\n",
      "99 Train accuracy: 0.97333336 Test_accurracy: 0.9801\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs)) # 원래의 이미지 사이즈\n",
    "y_test = mnist.test.labels\n",
    "n_epochs = 100\n",
    "batch_size = 150\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        # 60000 / 150\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            #-1 : 계산결과 나머지 : % 150\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs)) # 원래의 이미지 사이즈\n",
    "            \n",
    "            # 150x28x28\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch}) # 학습\n",
    "        acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch}) # 정확도\n",
    "        acc_test = accuracy.eval(feed_dict={X:X_test, y:y_test}) #\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test_accurracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding -> decoding\n",
    "# 끝글자 예측 \n",
    "tf.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 알파벳\n",
    "# 알파벳을 숫자로 인덱싱 -> 숫자를 문자로 변경가능\n",
    "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "            'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# 글자에 대한 인덱스 번호 구성\n",
    "# a:0 b:1 c:2 ... z:25\n",
    "# 문자를 숫자로 변경\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)} # 순서대로 번호지정\n",
    "\n",
    "dic_len = len(num_dic) #26\n",
    "seq_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.eye(26)[22]) # w\n",
    "print(np.eye(26)[14]) # o\n",
    "print(np.eye(26)[17]) # r \n",
    "print(np.eye(26)[3]) # d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩 : 신경망에서 범주형의 경우 원핫인코딩\n",
    "\n",
    "# 셀수 3\n",
    "# 셀당입력 26\n",
    "def make_batch(seq_data):\n",
    "    input_batch = [] # 3글자\n",
    "    target_batch = [] # 1글자\n",
    "    for seq in seq_data: # word\n",
    "        input = [num_dic[n] for n in seq[:-1]] # wor -> w:22, o:14, r:17 인코딩\n",
    "        target = num_dic[seq[-1]] # d:3\n",
    "        # 신경망에서 범주형 데이터는 원핫인코딩되야한다\n",
    "        # eye : 단위행렬 : 대각선1\n",
    "        input_batch.append(np.eye(dic_len)[input])# 단위행렬을 이용한 원핫원코딩\n",
    "        target_batch.append(target) # 3\n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "n_hidden = 128 # 가중치를 위한 히든\n",
    "total_epoch = 30\n",
    "n_step = 3 # 셀수\n",
    "n_input = n_class = dic_len # 26(a~z)\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input]) # 배치사이즈, 셀수, 셀당입력\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# FFNN을 위한 가중치 준비\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class])) # 128x26\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "# rnn으로 특징 추출( 셀당 가중치가 학습 )\n",
    "# control_state를 추가 : 장기기억이 가능하도록 : 변종이 많음\n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden) # 나가는 차수\n",
    "\n",
    "# 일부 계산회로 줄임 ( 랜덤 ) : 과적합 때문에\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5, seed=10)\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "# multiRNN : 상하로 RNN셀을 두번 쌓음( 더 정확한 학습 ) \n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])  \n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32) # ?x3x26 # 셀 3개가 결정\n",
    "# 배치사이즈x3x128(히든), 배치사이즈x128\n",
    "outputs = tf.transpose(outputs, [1,0,2]) # 3x배치사이즈x128\n",
    "outputs = outputs[-1] # 맨뒤  # 맨마지막셀  # 배치사이즈x128\n",
    "model = tf.matmul(outputs, W)+b # == dense와 동일하게 연산 : 배치사이즈x26\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "\n",
    "# RMSProp + momentum을 조정\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오전 종료 ==========================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001 cost = 4.531738\n",
      "Epoch 0002 cost = 3.467937\n",
      "Epoch 0003 cost = 1.893429\n",
      "Epoch 0004 cost = 1.725192\n",
      "Epoch 0005 cost = 0.934616\n",
      "Epoch 0006 cost = 0.798401\n",
      "Epoch 0007 cost = 0.774154\n",
      "Epoch 0008 cost = 0.728611\n",
      "Epoch 0009 cost = 0.643163\n",
      "Epoch 0010 cost = 0.572102\n",
      "Epoch 0011 cost = 0.245229\n",
      "Epoch 0012 cost = 0.477978\n",
      "Epoch 0013 cost = 0.388104\n",
      "Epoch 0014 cost = 0.248850\n",
      "Epoch 0015 cost = 0.110863\n",
      "Epoch 0016 cost = 0.086742\n",
      "Epoch 0017 cost = 0.235641\n",
      "Epoch 0018 cost = 0.107952\n",
      "Epoch 0019 cost = 0.196902\n",
      "Epoch 0020 cost = 0.141469\n",
      "Epoch 0021 cost = 0.118111\n",
      "Epoch 0022 cost = 0.054736\n",
      "Epoch 0023 cost = 0.033359\n",
      "Epoch 0024 cost = 0.077499\n",
      "Epoch 0025 cost = 0.025251\n",
      "Epoch 0026 cost = 0.013885\n",
      "Epoch 0027 cost = 0.043071\n",
      "Epoch 0028 cost = 0.065216\n",
      "Epoch 0029 cost = 0.018888\n",
      "Epoch 0030 cost = 0.037207\n",
      "\n",
      "==예측결과==\n",
      "입력값 :  ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'coo ', 'loa ', 'lov ', 'kis ', 'kin ']\n",
      "예측값 :  ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_batch, target_batch = make_batch(seq_data)# 변수둘 - 원핫인코딩 결과, 문자를 숫자로매핑\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost], #optimizer는 학습, cost는 에포크당 비용 출력\n",
    "                       feed_dict={X: input_batch, Y:target_batch})\n",
    "    print('Epoch', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))    \n",
    "    \n",
    "# model은 분산과 같은 형태로 예측\n",
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "prediction_check = tf.equal(prediction, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32)) # 정확도\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "# predict : 숫자 -> 디코딩\n",
    "predict, accuracy_val = sess.run([prediction, accuracy], feed_dict={X: input_batch, Y:target_batch})\n",
    "predict_words = []\n",
    "\n",
    "for idx, val in enumerate(seq_data):\n",
    "    # target의 알파벳을 번호로 결정\n",
    "    last_char = char_arr[predict[idx]] # inverse mapping 문자 -> 숫자, 숫자 -> 문자\n",
    "    predict_words.append(val[:3] + last_char) # 입력된 데이터 + 예측된 끝단어\n",
    "print(\"\\n==예측결과==\")\n",
    "print(\"입력값 : \", [w[:3]+ ' ' for w in seq_data])\n",
    "print('예측값 : ', predict_words)\n",
    "print('정확도 : ', accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "번역망 ( seq 2 seq )\n",
    "\"\"\"\n",
    "#email\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀연습사랑']\n",
    "\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "print(dic_len)\n",
    "seq_data = [['word', '단어'], ['wood', '나무'],\n",
    "            ['game', '놀이'], ['girl', '소녀'],\n",
    "            ['test', '연습'], ['love', '사랑']] \n",
    "\n",
    "# ==\n",
    "# 왼쪽망   : input(영어) => smoking gun, \n",
    "# 오른쪽망 : output(한), target(한)\n",
    "# 두 개의 망은 연결\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    for seq in seq_data:\n",
    "        input = [num_dic[n] for n in seq[0]] # w:25 o:17 r:20 d:6\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])] # SEP : start: 시작의 의미 비어있음 S:0 29:단 30:어\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')] # SEP : end: 번역의 종료지점(계속번역금지-공간) 29:단 30:어 E:1\n",
    "        input_batch.append(np.eye(dic_len)[input]) # 원핫인코딩\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        target_batch.append(target) # [0, 29, 30]\n",
    "    return input_batch, output_batch, target_batch\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Epoch:  0001 cost =  3.780606\n",
      "Epoch:  0002 cost =  2.590755\n",
      "Epoch:  0003 cost =  1.767363\n",
      "Epoch:  0004 cost =  1.220510\n",
      "Epoch:  0005 cost =  0.869404\n",
      "Epoch:  0006 cost =  0.471994\n",
      "Epoch:  0007 cost =  0.220139\n",
      "Epoch:  0008 cost =  0.205051\n",
      "Epoch:  0009 cost =  0.125327\n",
      "Epoch:  0010 cost =  0.091335\n",
      "Epoch:  0011 cost =  0.071131\n",
      "Epoch:  0012 cost =  0.186076\n",
      "Epoch:  0013 cost =  0.071023\n",
      "Epoch:  0014 cost =  0.029767\n",
      "Epoch:  0015 cost =  0.048297\n",
      "Epoch:  0016 cost =  0.052502\n",
      "Epoch:  0017 cost =  0.017864\n",
      "Epoch:  0018 cost =  0.039752\n",
      "Epoch:  0019 cost =  0.005683\n",
      "Epoch:  0020 cost =  0.013886\n",
      "Epoch:  0021 cost =  0.009827\n",
      "Epoch:  0022 cost =  0.003929\n",
      "Epoch:  0023 cost =  0.004667\n",
      "Epoch:  0024 cost =  0.023378\n",
      "Epoch:  0025 cost =  0.002879\n",
      "Epoch:  0026 cost =  0.003372\n",
      "Epoch:  0027 cost =  0.012189\n",
      "Epoch:  0028 cost =  0.002995\n",
      "Epoch:  0029 cost =  0.006330\n",
      "Epoch:  0030 cost =  0.004249\n",
      "Epoch:  0031 cost =  0.009292\n",
      "Epoch:  0032 cost =  0.007687\n",
      "Epoch:  0033 cost =  0.001981\n",
      "Epoch:  0034 cost =  0.001532\n",
      "Epoch:  0035 cost =  0.002851\n",
      "Epoch:  0036 cost =  0.002978\n",
      "Epoch:  0037 cost =  0.001602\n",
      "Epoch:  0038 cost =  0.002905\n",
      "Epoch:  0039 cost =  0.001875\n",
      "Epoch:  0040 cost =  0.001572\n",
      "Epoch:  0041 cost =  0.001582\n",
      "Epoch:  0042 cost =  0.001209\n",
      "Epoch:  0043 cost =  0.001114\n",
      "Epoch:  0044 cost =  0.001109\n",
      "Epoch:  0045 cost =  0.000655\n",
      "Epoch:  0046 cost =  0.001660\n",
      "Epoch:  0047 cost =  0.000203\n",
      "Epoch:  0048 cost =  0.004592\n",
      "Epoch:  0049 cost =  0.000967\n",
      "Epoch:  0050 cost =  0.001030\n",
      "Epoch:  0051 cost =  0.000908\n",
      "Epoch:  0052 cost =  0.000312\n",
      "Epoch:  0053 cost =  0.001036\n",
      "Epoch:  0054 cost =  0.000265\n",
      "Epoch:  0055 cost =  0.000378\n",
      "Epoch:  0056 cost =  0.000238\n",
      "Epoch:  0057 cost =  0.000494\n",
      "Epoch:  0058 cost =  0.000286\n",
      "Epoch:  0059 cost =  0.000496\n",
      "Epoch:  0060 cost =  0.000401\n",
      "Epoch:  0061 cost =  0.000928\n",
      "Epoch:  0062 cost =  0.000392\n",
      "Epoch:  0063 cost =  0.000300\n",
      "Epoch:  0064 cost =  0.000968\n",
      "Epoch:  0065 cost =  0.000133\n",
      "Epoch:  0066 cost =  0.000379\n",
      "Epoch:  0067 cost =  0.000493\n",
      "Epoch:  0068 cost =  0.000199\n",
      "Epoch:  0069 cost =  0.000483\n",
      "Epoch:  0070 cost =  0.000711\n",
      "Epoch:  0071 cost =  0.002350\n",
      "Epoch:  0072 cost =  0.000272\n",
      "Epoch:  0073 cost =  0.001034\n",
      "Epoch:  0074 cost =  0.000790\n",
      "Epoch:  0075 cost =  0.000402\n",
      "Epoch:  0076 cost =  0.000275\n",
      "Epoch:  0077 cost =  0.000269\n",
      "Epoch:  0078 cost =  0.000712\n",
      "Epoch:  0079 cost =  0.000240\n",
      "Epoch:  0080 cost =  0.000811\n",
      "Epoch:  0081 cost =  0.000750\n",
      "Epoch:  0082 cost =  0.000423\n",
      "Epoch:  0083 cost =  0.000248\n",
      "Epoch:  0084 cost =  0.000443\n",
      "Epoch:  0085 cost =  0.000282\n",
      "Epoch:  0086 cost =  0.000247\n",
      "Epoch:  0087 cost =  0.000202\n",
      "Epoch:  0088 cost =  0.000389\n",
      "Epoch:  0089 cost =  0.000279\n",
      "Epoch:  0090 cost =  0.000298\n",
      "Epoch:  0091 cost =  0.000119\n",
      "Epoch:  0092 cost =  0.000285\n",
      "Epoch:  0093 cost =  0.000356\n",
      "Epoch:  0094 cost =  0.000080\n",
      "Epoch:  0095 cost =  0.000440\n",
      "Epoch:  0096 cost =  0.000224\n",
      "Epoch:  0097 cost =  0.000138\n",
      "Epoch:  0098 cost =  0.000405\n",
      "Epoch:  0099 cost =  0.000578\n",
      "Epoch:  0100 cost =  0.000396\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128 # 셀당출력\n",
    "total_epoch = 100\n",
    "n_class = n_input = dic_len # 41\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input]) # 셀당입력x셀수x배치 : 6x4(알파벳이 4개)x41\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input]) # 6x3x41\n",
    "targets = tf.placeholder(tf.int64, [None, None]) # 6x3\n",
    "\n",
    "# 연결지점을 확인하시오\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden) # 출력차수 128\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5, seed=100)\n",
    "    \n",
    "    # 셀수 4개로 결정\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n",
    "    \n",
    "# output : 6x4x128\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden) # 128\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    # 셀이 3개로 결정\n",
    "    # 연결부 : initial_state\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32) \n",
    "    \n",
    "# output : 6x3x128\n",
    "model = tf.layers.dense(outputs, n_class, activation=None) # 3x41\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('-')\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                      feed_dict={enc_input: input_batch,\n",
    "                                dec_input: output_batch,\n",
    "                                targets : target_batch})\n",
    "    print('Epoch: ', '%04d' % (epoch + 1), 'cost = ', '{:.6f}'.format(loss) )\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word ->  단어\n",
      "love ->  사랑\n",
      "abcd ->  나무단\n",
      "wodr ->  나무\n",
      "loev ->  사랑\n"
     ]
    }
   ],
   "source": [
    "def translate(word):\n",
    "    seq_data = [word, 'P' * len(word)] # 번역된 데이터 생성 ( 임의의 글자를 P )\n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    prediction = tf.argmax(model, 2) # 예측값 6x3x41 (41개의 단어중 하나로)\n",
    "    result = sess.run(prediction, feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch})\n",
    "    \n",
    "    # 번역할 단어 41개중 하나로 예측\n",
    "    decoded = [char_arr[i] for i in result[0]] # 숫자를 문자로 매핑\n",
    "    end = decoded.index('E') # E를 예측\n",
    "    translated = ''.join(decoded[:end]) # E앞단까지 단어 조합\n",
    "    return translated\n",
    "\n",
    "print('word -> ', translate('word'))\n",
    "print('love -> ', translate('love'))\n",
    "print('abcd -> ', translate('abcd'))\n",
    "\n",
    "\n",
    "print('wodr -> ', translate('wodr'))\n",
    "print('loev -> ', translate('loev'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "문제 : 다음 인코딩 된 값을 2개의 특성을 가진 데이터로 변환해 보시오\n",
    "h=[1,0,0,0]\n",
    "e=[0,1,0,0]\n",
    "l=[0,0,1,0]\n",
    "o=[0,0,0,1]\n",
    "# hello 데이터에 대하여 2개의 특성으로 추출하시오\n",
    "# 5x2로 출력되도록 해보시오\n",
    "\n",
    "결정해야될것 : 배치사이즈, 셀수, 셀당출력(아웃풋) : 1, 5, 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 4)\n",
      "(1, 5, 2)\n",
      "(1, 2)\n",
      "array([[-0.63022363,  0.6946867 ]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "h=[1,0,0,0]\n",
    "e=[0,1,0,0]\n",
    "l=[0,0,1,0]\n",
    "o=[0,0,0,1]\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "with tf.variable_scope('hello') as scope: # 클로즈 자동호출 with # 그래프내부 네임스페이스\n",
    "    hidden_size = 2\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    x_data = np.array([[h, e, l, l, o]], dtype=np.float32) # 5x4\n",
    "    print(x_data.shape)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(outputs.shape)\n",
    "    print(states.shape)\n",
    "    pp.pprint(states.eval())\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]],\n",
      "\n",
      "       [[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]]], dtype=float32)\n",
      "array([[[-0.6651914 , -0.6653228 ],\n",
      "        [-0.4614807 ,  0.4314423 ],\n",
      "        [-0.2528372 , -0.86558074],\n",
      "        [-0.602575  , -0.12148489],\n",
      "        [-0.60226464, -0.6724281 ]],\n",
      "\n",
      "       [[-0.29223335,  0.31307113],\n",
      "        [-0.51014775, -0.7536512 ],\n",
      "        [-0.5723368 , -0.35576442],\n",
      "        [-0.47496527, -0.62727904],\n",
      "        [-0.5442506 , -0.42532873]],\n",
      "\n",
      "       [[-0.3952672 , -0.58989143],\n",
      "        [-0.53741306, -0.41026315],\n",
      "        [-0.3959082 ,  0.31917316],\n",
      "        [-0.17909819, -0.17932078],\n",
      "        [-0.43936962, -0.5650803 ]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3 batches 'hello', 'eolll', 'lleel' 2개의 특성으로 embedding 해보시오\n",
    "\"\"\"\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "x_data = np.array([[h,e,l,l,o], \n",
    "                   [e,o,l,l,l], \n",
    "                   [l,l,e,e,l]], dtype=np.float32)\n",
    "\n",
    "\n",
    "pp.pprint(x_data)\n",
    "\n",
    "with tf.variable_scope('hello2') as scope:\n",
    "    hidden_size = 2\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "\n",
    "\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국', '의', '무궁', '한', '발전', '과', '세계', '를', '이끄는', '지도', '국가', '가', '되길', '희망', '합니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 텍스트마이닝\n",
    "from konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "token=okt.morphs('대한민국의 무궁한 발전과 세계를 이끄는 지도 국가가 되길 희망합니다.')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'대한민국': 0, '의': 1, '무궁': 2, '한': 3, '발전': 4, '과': 5, '세계': 6, '를': 7, '이끄는': 8, '지도': 9, '국가': 10, '가': 11, '되길': 12, '희망': 13, '합니다': 14, '.': 15}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원핫 인코딩\n",
    "word2index={}\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index) # 키 데이터\n",
    "print(word2index)\n",
    "\n",
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0] * (len(word2index))\n",
    "    index = word2index[word]\n",
    "    one_hot_vector[index]=1 # 지정단어 원핫 인코딩\n",
    "    return one_hot_vector\n",
    "one_hot_encoding(\"세계\", word2index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
